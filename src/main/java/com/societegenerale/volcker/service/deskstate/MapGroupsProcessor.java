package com.societegenerale.volcker.service.deskstate;

import static org.apache.spark.sql.functions.broadcast;

import com.societegenerale.volcker.entity.DeskExposure;
import com.societegenerale.volcker.entity.DeskState;
import com.societegenerale.volcker.ulti.SparkTemplate;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.KeyValueGroupedDataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.streaming.GroupStateTimeout;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;
import org.springframework.stereotype.Service;

@Service("mapGroups")
public class MapGroupsProcessor extends SparkTemplate {

  @Override
  public void job() throws Exception {

    // Trader â†’ Desk mapping
    Dataset<Row> traderDesk = spark.read()
        .option("header", true)
        .csv("input/trader_desk.csv")
        .cache();

    traderDesk.count();

    // Trade schema
    StructType tradeSchema = new StructType()
        .add("tradeId", DataTypes.StringType)
        .add("tradeTime", DataTypes.TimestampType)
        .add("traderId", DataTypes.StringType)
        .add("instrumentId", DataTypes.StringType)
        .add("instrumentType", DataTypes.StringType)
        .add("side", DataTypes.StringType)
        .add("quantity", DataTypes.createDecimalType(18, 2))
        .add("price", DataTypes.createDecimalType(18, 2))
        .add("notional", DataTypes.createDecimalType(18, 2));

    // Streaming ingestion
    Dataset<Row> tradeStream = spark.readStream()
        .schema(tradeSchema)
        .option("header", true)
        .csv("input/mapgroups"); // æ–‡ä»¶å¤¹è·¯å¾„

    // Enrich with deskId
    Dataset<Row> enriched = tradeStream.join(
        broadcast(traderDesk),
        tradeStream.col("traderId").equalTo(traderDesk.col("traderId")),
        "left"
    ).drop(traderDesk.col("traderId"));
    
    // Add watermark for event-time processing
    Dataset<Row> withWatermark = enriched.withWatermark("tradeTime", "10 minutes");

    // Group by deskId and apply stateful aggregation
    KeyValueGroupedDataset<String, Row> grouped =
        withWatermark.groupByKey(
            (MapFunction<Row, String>) r -> r.getAs("deskId"),
            Encoders.STRING()
        );
    // col("deskId") è¿”å› Column
    // df.groupBy(col("deskId")).count() //Dataset
    // ä» Row åˆ‡å› Trade
    // ğŸš©strong type
    // .as(Encoders.bean(TradeWithDesk.class)) // å¿…é¡» Serializableï¼Œå¿…é¡»æ— å‚æ„é€ 
    // .groupByKey(t -> t.getDeskId(), Encoders.STRING())

    grouped.mapGroupsWithState(
        (desk, trade, state) -> {

          return null;
        },
        Encoders.bean(DeskState.class),      // çŠ¶æ€ï¼ˆStateï¼‰çš„ç¼–ç å™¨
        Encoders.bean(DeskExposure.class),   // è¾“å‡ºç»“æœçš„ç¼–ç å™¨
        GroupStateTimeout.EventTimeTimeout() // çŠ¶æ€è¶…æ—¶ç­–ç•¥ - åŸºäºäº‹ä»¶æ—¶é—´
    );
    Dataset<DeskExposure> deskExposure = grouped.mapGroupsWithState(
        new DeskExposureStateRowFn(),        // çŠ¶æ€æ›´æ–°é€»è¾‘çš„æ ¸å¿ƒå®ç°ç±»
        Encoders.bean(DeskState.class),      // çŠ¶æ€ï¼ˆStateï¼‰çš„ç¼–ç å™¨
        Encoders.bean(DeskExposure.class),   // è¾“å‡ºç»“æœçš„ç¼–ç å™¨
        GroupStateTimeout.EventTimeTimeout() // çŠ¶æ€è¶…æ—¶ç­–ç•¥ - åŸºäºäº‹ä»¶æ—¶é—´
    );
//    deskExposure.show();

    // Output to console
    deskExposure.writeStream()
        .outputMode("update")
        .format("console")
        .option("truncate", false)
        .start()
        .awaitTermination();
  }
}
